{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/ia/m9/zk/iam9zkyzqebnf_okxipihkgjwnw.jpeg\" />\n",
    "    \n",
    "**<center>[mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course** </center><br>\n",
    "Author: [Yury Kashnitsky](https://yorko.github.io) (@yorko). Edited by Anna Tarelina (@feuerengel).[mlcourse.ai](https://mlcourse.ai) is powered by [OpenDataScience (ods.ai)](https://ods.ai/) © 2017—2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment #3. Optional part. Task </center> <a class=\"tocSkip\">\n",
    "## <center>Implementation of the decision tree algorithm </center><a class=\"tocSkip\">\n",
    "\n",
    "If you want to truly understand the algorithm that you are using, it's good to implement it from scratch. Have to say, it's not necessary if you just want to apply an algorithm in practice. However, if you love math, algorithms and programming, you'll enjoy this task.  \n",
    "<img src='../../_static/img/quote_feynman.jpg' width=50%>\n",
    "\n",
    "### Your task is to:\n",
    " 1. write code and perform computations in the cells below;\n",
    " 2. choose answers in the [webform](https://docs.google.com/forms/d/1SYwUD0Yx_bcykq6EqFQ4Ug0KaQWG4L7bAlL5yGedZnw).\n",
    "    \n",
    "\n",
    "*If you are sure that something is not 100% correct with the assignment/solution, please leave your feedback via the mentioned webform ↑*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# if seaborn is not yet installed, run `pip install seaborn` in terminal\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import (\n",
    "    fetch_california_housing,\n",
    "    load_digits,\n",
    "    make_classification,\n",
    "    make_regression,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sharper plots\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix `random_state` (a.k.a. random seed) beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Implement the class `DecisionTree`**\n",
    "**Specification:**\n",
    "- the class is inherited from `sklearn.BaseEstimator`;\n",
    "- class constructor has the following parameters: \n",
    "    - `max_depth` - maximum depth of the tree (`numpy.inf` by default); \n",
    "    - `min_samples_split` - the minimum number of instances in a node for a splitting to be done (2 by default); \n",
    "    - `criterion` - split criterion ('gini' or 'entropy' for classification, 'variance' or 'mad_median' for regression; 'gini' by default);\n",
    "- the class has several methods: `fit`, `predict` and `predict_proba`;\n",
    "- the`fit` method takes the matrix of instances `X` and a target vector `y` (`numpy.ndarray` objects) and returns an instance of the class `DecisionTree` representing the decision tree trained on the dataset `(X, y)` according to parameters set in the constructor; \n",
    "- the `predict_proba` method takes the matrix of instances `X` and returns the matrix `P` of a size `X.shape[0] x K`, where `K` is the number of classes and $p_{ij}$ is the probability of an instance in $i$-th row of `X` to belong to class $j \\in \\{1, \\dots, K\\}$.\n",
    "- the `predict` method takes the matrix of instances `X` and returns a prediction vector; in case of classification, prediction for an instance $x_i$ falling into leaf $L$ will be the class, mostly represented among instances in $L$. In case of regression, it'll be the mean value of targets for all instances in leaf $L$.\n",
    "\n",
    "A note on `criterion`: this is the functional to be maximized to find an optimal partition at a given node has the form $Q(X, j, t) = F(X) - \\dfrac{|X_l|}{|X|} F(X_l) - \\dfrac{|X_r|}{|X|} F(X_r),$ where $X$ are samples at a given node, $X_l$ and $X_r$ are partitions of samples $X$ into two parts with the following condition $[x_j < t]$, and $F(X)$ is a partition criterion.\n",
    "    \n",
    "For classification: let $p_i$ be the fraction of the instances of the $i$-th class in the dataset $X$.\n",
    "- 'gini': Gini impurity $F(X) = 1 -\\sum_{i = 1}^K p_i^2$.\n",
    "- 'entropy': Entropy $F(X) = -\\sum_{i = 1}^K p_i \\log_2(p_i)$.\n",
    "    \n",
    "For regression: $y_j = y(x_j)$ - is a target for an instance $x_j$, $y = (y_1, \\dots, y_{|X|})$ - is a target vector.\n",
    "- 'variance': Variance (mean quadratic deviation from average) $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}(y_j - \\dfrac{1}{|X|}\\sum_{x_i \\in X}y_i)^2$\n",
    "- 'mad_median': Mean deviation from the median $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}|y_j - \\mathrm{med}(y)|$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return -np.dot(p, np.log2(p))\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    p = [len(y[y == k]) / len(y) for k in np.unique(y)]\n",
    "    return 1 - np.dot(p, p)\n",
    "\n",
    "\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def mad_median(y):\n",
    "    med = np.median(y)\n",
    "    deviations = np.abs(med - y)\n",
    "    return np.mean(deviations)\n",
    "\n",
    "criteria_dict = {\n",
    "    'entropy': entropy,\n",
    "    'gini': gini,\n",
    "    'variance': variance,\n",
    "    'mad_median': mad_median\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Node` class implements a node in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_idx=0, threshold=0, labels=None, left=None, right=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.labels = labels\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the function for calculating a prediction in a leaf. For regression, let's take the mean for all values in a leaf, for classification - the most popular class in leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)\n",
    "\n",
    "def regression_leaf(y):\n",
    "    return np.mean(y)\n",
    "\n",
    "\n",
    "def classification_leaf(y):\n",
    "    y = np.asarray(y)\n",
    "    return np.bincount(y).argmax()\n",
    "\n",
    "\n",
    "class DecisionTree(BaseEstimator):\n",
    "    def __init__(self, max_depth=np.inf, min_samples_split=2, criterion=\"gini\", debug=False):\n",
    "        params = {\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"criterion\": criterion,\n",
    "            \"debug\": debug,\n",
    "        }\n",
    "\n",
    "        for param_name, param_value in params.items():\n",
    "            setattr(self, param_name, param_value)\n",
    "\n",
    "        super(DecisionTree, self).set_params(**params)\n",
    "\n",
    "        self._criterion_function = criteria_dict[criterion]\n",
    "\n",
    "        if criterion in [\"variance\", \"mad_median\"]:\n",
    "            self._leaf_value = regression_leaf\n",
    "        else:\n",
    "            self._leaf_value = classification_leaf\n",
    "\n",
    "    def _information_gain(self, X, y, threshold, feature_idx):\n",
    "        mask = X[:, feature_idx] < threshold\n",
    "        n_left = np.sum(mask)\n",
    "        n_right = self._n_samples - n_left\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        return (\n",
    "            self._current_criterion\n",
    "            - (n_left * self._criterion_function(y[mask]) + n_right * self._criterion_function(y[~mask]))\n",
    "            / self._n_samples\n",
    "        )\n",
    "            \n",
    "    def _build_tree(self, X, y, depth=1):\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return Node(labels=y)\n",
    "\n",
    "        self._current_criterion = self._criterion_function(y)\n",
    "        \n",
    "        best_gain = 0\n",
    "        best_threshold = None\n",
    "        best_feature_idx = None\n",
    "        best_mask = None\n",
    "        if depth < self.max_depth and self._n_samples >= self.min_samples_split:\n",
    "            for feature_idx in range(self._n_features):\n",
    "                threshold_values = np.unique(X[:, feature_idx])\n",
    "                information_gains = [self._information_gain(X, y, threshold, feature_idx) for threshold in threshold_values]\n",
    "                best_gain_idx = np.nanargmax(information_gains)\n",
    "\n",
    "                if information_gains[best_gain_idx] > best_gain:\n",
    "                    best_gain = information_gains[best_gain_idx]\n",
    "                    best_threshold = threshold_values[best_gain_idx]\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_mask = X[:, feature_idx] < best_threshold\n",
    "\n",
    "        if best_gain > 0:\n",
    "            return Node(\n",
    "                feature_idx=best_feature_idx,\n",
    "                threshold=best_threshold,\n",
    "                left=self._build_tree(X[best_mask, :], y[best_mask], depth + 1),\n",
    "                right=self._build_tree(X[~best_mask, :], y[~best_mask], depth + 1)\n",
    "            )\n",
    "        else:\n",
    "            return Node(labels=y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._n_samples = X.shape[0]\n",
    "        self._n_features = X.shape[1]\n",
    "        self.root = self._build_tree(X, y)\n",
    "        \n",
    "        if self.criterion in ['gini', 'entropy']:\n",
    "            self._n_classes = len(np.unique(y))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict_helper(self, x):\n",
    "        node = self.root\n",
    "        while node.labels is None:\n",
    "            if x[node.feature_idx] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return self._leaf_value(node.labels)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_helper(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the implemented algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `digits` using the method `load_digits`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, and `random_state=17`. Try to train shallow decision trees and make sure that gini and entropy criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0\n",
      " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "\n",
    "tree_gini = DecisionTree(max_depth=2, criterion='gini')\n",
    "tree_gini.fit(X_train, y_train)\n",
    "\n",
    "pred_gini = tree_gini.predict(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_entropy = DecisionTree(max_depth = 2, criterion='entropy')\n",
    "tree_entropy.fit(X_train, y_train)\n",
    "pred_entropy = tree.predict(X_valid)\n",
    "accuracy_entropy = accuracy_score(y_valid, pred_entropy)\n",
    "\n",
    "accuracy_gini, accuracy_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 1.0, 21, 2.0)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_gini.root.feature_idx, tree_gini.root.threshold, tree_entropy.root.feature_idx, tree_entropy.root.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use range(3, 11), for criterion use {'gini', 'entropy'}. Quality measure is `scoring`='accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `accuracy` for criteria `gini` and `entropy` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Question 1.</font> Choose all correct statements:**\n",
    "1. Optimal value of the `max_depth` parameter is on the interval [4, 9] for both criteria.\n",
    "2. Created plots have no intersection on the interval [3, 10]\n",
    "3. Created plots intersect each other only once on the interval [3, 10].\n",
    "4. The best quality for `max_depth` on the interval [3, 10] is reached using `gini` criterion .\n",
    "5. Accuracy is strictly increasing at least for one of the criteria, when `max_depth` is also increasing on the interval [3, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Question 2.</font> What are the optimal values for max_depth and criterion parameters?**\n",
    "1. max_depth = 7, criterion = 'gini';\n",
    "2. max_depth = 7, criterion = 'entropy';\n",
    "3. max_depth = 10, criterion = 'entropy';\n",
    "4. max_depth = 10, criterion = 'gini';\n",
    "5. max_depth = 9, criterion = 'entropy';\n",
    "6. max_depth = 9, criterion = 'gini';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train decision tree on `(X_train, y_train)` using the optimal values of `max_depth` and `criterion`. Compute class probabilities for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given matrix, compute the mean class probabilities for all instances in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Question 3.</font> What is the maximum probability in a resulted vector?**\n",
    "1. 0.127\n",
    "2. 0.118\n",
    "3. 1.0\n",
    "4. 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `boston` using the method `load_boston`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, `random_state=17`. Try to train shallow regression decision trees and make sure that `variance` and `mad_median` criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use `range(2, 9)`, for `criterion` use {'variance', 'mad_median'}. Quality measure is `scoring`='neg_mean_squared_error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `neg_mean_squared_error` for criteria `variance` and `mad_median` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here (read-only in a JupyterBook, pls run jupyter-notebook to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Question 4.</font> Choose all correct statements:**\n",
    "1. Created plots have no intersection on the interval [2, 8].\n",
    "2. Created plots intersect each other only once on the interval [2, 8].\n",
    "3. Optimal value of the `max_depth` for each of the criteria is on the border of the interval [2, 8].\n",
    "4. The best quality at `max_depth` on the interval [2, 8] is reached using `mad_median` criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Question 5.</font> What are the optimal values for `max_depth` and `criterion` parameters?**\n",
    "1. max_depth = 9, criterion = 'variance';\n",
    "2. max_depth = 5, criterion = 'mad_median';\n",
    "3. max_depth = 4, criterion = 'variance';\n",
    "4. max_depth = 2, criterion = 'mad_median';\n",
    "5. max_depth = 4, criterion = 'mad_median';\n",
    "6. max_depth = 5, criterion = 'variance'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "name": "lesson4_part2_Decision_trees.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
